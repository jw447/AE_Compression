# -*- coding: utf-8 -*-
"""Autoencoder2201.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p7TM30hEGO1zZMFLYIyRvTRaiK9Ac-Nb
"""

import tensorflow as tf
import numpy as np  
import matplotlib.pyplot as plt  
import os
import time
# import progressbar

start_time = time.time()

sess = tf.Session()

f = open("./sedovappend_training.bin", "rb")

name = "sedovappend_training.bin"

print(name)  # Name of file

data = f.read()  # Contents of file
f.close()

print("The name of the file for training is:", name)

record_bytes = len(data)  # Length of data
print("Length of data:", len(data))
reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)

# Convert the data to a vector of numbers and store it in data_num
if(name[-4:] == ".txt"):
  data_num = data.decode().split('\n')
  if(data_num[len(data_num) - 1] == ""):
    data_num.remove("")
  for i in range(len(data_num)):
    data_num[i] = float(data_num[i])
  data_num = np.array(data_num)
else:
  data_node = tf.placeholder(tf.string)
  record_bytes = tf.decode_raw(data_node, tf.float64)
  data_num = sess.run(record_bytes, {data_node: data})
print("data_num =", data_num)

size = tf.size(data_num)  # Size of data_num (the vector of numbers)
data_num_size = sess.run(size)
print("data_num_size =", data_num_size)

original_data_num = np.copy(data_num)  # Copy data_num (by value, not reference) and store the copy in original_data_num
modifications = [1 for x in range(data_num_size)]  # Initialize modifications with ones
error_range = [1 for x in range(data_num_size)]  # array to record the RE for each data point

data_min = 0.1
data_max = 1

# Make the values in data_num be in the range [0.1, 1], and store the amount that each element was multiplied or divided by in modifications
for i in range(data_num.size):
  if data_num[i] == 0:
    continue
  if data_num[i] < 0:
    data_num[i] = data_num[i] * -1
    modifications[i] = modifications[i] * -1
  while data_num[i] < data_min:
    data_num[i] = data_num[i] * 10
    modifications[i] = modifications[i] * 10
  while data_num[i] > data_max:
    data_num[i] = data_num[i] / 10
    modifications[i] = modifications[i] / 10

print("data_num (after modification) =", data_num)
print("original_data_num =", original_data_num)

training_epochs = 2000
batch_size = 16
n_input = 200

# Error bound definitions
error_0 = 0    # if error = 0
error_A = 0    # if 0 < error < 0.0001(0.01%)
error_B = 0    # if 0.0001 < error < 0.001(0.1%)
error_C = 0    # if 0.001 < error < 0.01(1%)
error_D = 0    # if 0.01 < error < 0.1(10%)
error_E = 0    # if 0.1 < error < 1(100%)
error_F = 0    # if 1 < error
error_sum = 0

X = tf.placeholder("float", [None, None])
Y = tf.placeholder("float", [None, n_input])
Z = tf.placeholder("float", [None, n_input])
dropout_prob = tf.placeholder("float")

n_hidden_1 = int(n_input / 4)
n_hidden_2 = int(n_hidden_1 / 5)
n_hidden_3 = int(n_hidden_2 / 5)

weights = {
    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),
    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),
    'encoder_h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),
    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_2])),
    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),
    'decoder_h3': tf.Variable(tf.random_normal([n_hidden_1, n_input]))
}

biases = {
    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),
    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),
    'encoder_b3': tf.Variable(tf.random_normal([n_hidden_3])),
    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_2])),
    'decoder_b2': tf.Variable(tf.random_normal([n_hidden_1])),
    'decoder_b3': tf.Variable(tf.random_normal([n_input]))
}

layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(X, weights['encoder_h1']), biases['encoder_b1']))
layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(X, weights['encoder_h2']), biases['encoder_b2']))
layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(X, weights['encoder_h3']), biases['encoder_b3']))

layer_1d = tf.nn.sigmoid(tf.add(tf.matmul(X, weights['decoder_h1']), biases['decoder_b1']))
layer_2d = tf.nn.sigmoid(tf.add(tf.matmul(X, weights['decoder_h2']), biases['decoder_b2']))
layer_3d = tf.nn.sigmoid(tf.add(tf.matmul(X, weights['decoder_h3']), biases['decoder_b3']))

def encoder(x):
  layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))
  layer_1 = tf.nn.dropout(layer_1, dropout_prob)
  layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))
  layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['encoder_h3']), biases['encoder_b3']))
  return layer_3

def decoder(x):
  layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))
  layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))
  layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['decoder_h3']), biases['decoder_b3']))
  return layer_3

encoder_op = encoder(X)
decoder_op = decoder(encoder_op)

y_pred = decoder_op
y_true = X
y_orig = Y

delta = y_true - y_pred
delta_orig = Y - Z

cost = tf.reduce_mean(tf.pow(delta, 2))
cost_orig = tf.reduce_mean(tf.pow(delta_orig, 2))

'''global_step = tf.Variable(0, trainable=False)
orig_learning_rate = 0.025
learning_rate = tf.train.exponential_decay(orig_learning_rate, global_step, 2000000, 0.94)'''
learning_rate = 0.00075
#optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step)
optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)

if True:
  init = tf.global_variables_initializer()
  sess.run(init)
  
  total_batch = int(data_num_size / (n_input * batch_size))
  batch_xs = [[0 for x in range(n_input)] for y in range(batch_size)]  # Initialize batch_xs to be filled with zeros
  original_values = [[0 for x in range(n_input)] for y in range(batch_size)]  # Initialize original_values to be filled with zeros

  for epoch in range(training_epochs):
    batch_xs = [[0 for x in range(n_input)] for y in range(batch_size)]
    original_values = [[0 for x in range(n_input)] for y in range(batch_size)]
    if epoch % 1000 == 0:
      print("Epoch %4d" % epoch, end = '\t')
    
    index = 0  # Tracks the index of data_num, the vector of numbers
    
    for i in range(total_batch):
      new_y = 0
      if i == (total_batch - 1):
        new_y = int(data_num_size / n_input) - (batch_size * i)
        if data_num_size % n_input != 0:
          new_y += 1
        batch_xs = [[0 for x in range(n_input)] for y in range(new_y)]
        original_values = [[0 for x in range(n_input)] for y in range(new_y)]
      temp_batch_size = batch_size
      if i == (total_batch - 1):
        temp_batch_size = new_y
      # Put the next (batch_size * n_input) numbers from data_num into batch_xs and the next (batch_size * n_input) numbers from original_data_num into original_values
      for j in range(temp_batch_size):
        for k in range(n_input):
          if index < data_num_size:
            batch_xs[j][k] = data_num[index]
            original_values[j][k] = original_data_num[index]
            index += 1
      
      _ = sess.run(optimizer, {X: batch_xs, dropout_prob: 0.6})
      
      if epoch % 1000 == 0 or epoch == (training_epochs - 1):
        p = sess.run(y_pred, {X: batch_xs, dropout_prob: 0.6})
        
        # For each predicted value, undo the modification that had been done on the original value of that prediction
        for r in range(np.size(p, 0)):
          for s in range(np.size(p, 1)):
            if ((i * batch_size * n_input) + (r * np.size(p, 1)) + s) < data_num_size:
              p[r][s] = p[r][s] / modifications[((i * batch_size * n_input) + (r * np.size(p, 1)) + s)]
    
        if epoch == (training_epochs - 1):
          c, d, t = sess.run([cost_orig, delta_orig, y_orig], feed_dict={X: batch_xs, Y: original_values, Z: p})
          print()
          temp_batch_size = batch_size
          if i == (total_batch - 1):
            temp_batch_size = new_y
          for a in range(temp_batch_size):
            for b in range(n_input):
              if b == (n_input - 1):
                if(i * batch_size * n_input + a * n_input + b) < data_num_size:
                  print("Epoch %4d\tInput Unit: %d\tt: %.8f\tp: %.8f\td: %.8f\tError: %.8f\tCost: %.16f" % (epoch, ((i * batch_size) + a), t[a][b], p[a][b], d[a][b], (abs(d[a][b])/t[a][b]), c))

        if epoch % 1000 == 0 or epoch == (training_epochs - 1):
          c, d, t = sess.run([cost_orig, delta_orig, y_orig], feed_dict={X: batch_xs, Y: original_values, Z: p})
          print("Batch", i, "- Cost: ", "{:.9f}".format(c), end = '\t')
          
          temp_batch_size = batch_size
          if i == (total_batch - 1):
            temp_batch_size = new_y
          for a1 in range(temp_batch_size):
            for b1 in range(n_input):
              if (i * batch_size * n_input + a1 * n_input + b1) < data_num_size:
                error_range[i*batch_size*n_input + a1*n_input+b1] = (abs(d[a1][b1])/t[a1][b1])
                error_sum = error_sum + error_range[i*batch_size*n_input + a1*n_input+b1]
                if(error_range[i*batch_size*n_input + a1*n_input+b1] == 0):
                  error_0 = error_0 + 1
                if(0 < error_range[i*batch_size*n_input + a1*n_input+b1] < 0.0001):
                  error_A = error_A + 1
                if(0.0001 < error_range[i*batch_size*n_input + a1*n_input+b1] < 0.001):
                  error_B = error_B + 1
                if(0.001 < error_range[i*batch_size*n_input + a1*n_input+b1] < 0.01):
                  error_C = error_C + 1
                if(0.01 < error_range[i*batch_size*n_input + a1*n_input+b1] < 0.1):
                  error_D = error_D + 1
                if(0.1 < error_range[i*batch_size*n_input + a1*n_input+b1] < 1):
                  error_E = error_E + 1
                if(1 < error_range[i*batch_size*n_input + a1*n_input+b1] ):
                  error_F = error_F + 1
                
    if epoch % 1000 == 0 or epoch == (training_epochs - 1):
      print("For the whole data set, Error_0: %.8f\tError_A: %.8f\tError_B: %.8f\tError_C: %.8f\tError_D: %.8f\tError_E: %.8f\tError_F: %.8f\tError_mean: %.8f\t" % ((error_0/data_num.size),(error_A/data_num.size),(error_B/data_num.size),(error_C/data_num.size),(error_D/data_num.size),(error_E/data_num.size),(error_F/data_num.size),(error_sum/data_num.size)))
      
      if epoch == (training_epochs - 1):
        training_error = error_sum / data_num.size
      
      # Reset the values of the error variables
      error_0 = 0    # if error = 0
      error_A = 0    # if 0 < error < 0.0001(0.01%)
      error_B = 0    # if 0.0001 < error < 0.001(0.1%)
      error_C = 0    # if 0.001 < error < 0.01(1%)
      error_D = 0    # if 0.01 < error < 0.1(10%)
      error_E = 0    # if 0.1 < error < 1(100%)
      error_F = 0    # if 1 < error
      error_sum = 0 
    
print("\nValidation")

f = open("./sedovappend_testing.bin", "rb")

name = "sedovappend_testing.bin.bin"

print(name)  # Name of file

data = f.read()  # Contents of file
f.close()


record_bytes = len(data)
print(len(data))
reader=tf.FixedLengthRecordReader(record_bytes=record_bytes)

# Convert the data to a vector of numbers and store it in data_num
if(name[-4:] == ".txt"):
  data_num = data.decode().split('\n')
  if(data_num[len(data_num) - 1] == ""):
    data_num.remove("")
  for i in range(len(data_num)):
    data_num[i] = float(data_num[i])
  data_num = np.array(data_num)
else:
  data_node = tf.placeholder(tf.string)
  record_bytes = tf.decode_raw(data_node, tf.float64)
  data_num = sess.run(record_bytes, {data_node: data})
print("data_num =", data_num)

size = tf.size(data_num)  # Size of data_num (the vector of numbers)
data_num_size = sess.run(size)
print("data_num_size =", data_num_size)

original_data_num = np.copy(data_num)  # Copy data_num (by value, not reference) and store the copy in original_data_num
modifications = [1 for x in range(data_num_size)]  # Initialize modifications with ones
error_range = [1 for x in range(data_num_size)]  # array to record the RE for each data point

# Make the values in data_num be in the range [0.1, 1], and store the amount that each element was multiplied or divided by in modifications
for i in range(data_num.size):
  if data_num[i] == 0:
    continue
  if data_num[i] < 0:
    data_num[i] = data_num[i] * -1
    modifications[i] = modifications[i] * -1
  while data_num[i] < data_min:
    data_num[i] = data_num[i] * 10
    modifications[i] = modifications[i] * 10
  while data_num[i] > data_max:
    data_num[i] = data_num[i] / 10
    modifications[i] = modifications[i] / 10

print("data_num (after modification) =", data_num)
print("original_data_num =", original_data_num)

if True:
  total_batch = int(data_num_size / (n_input * batch_size))
  batch_xs = [[0 for x in range(n_input)] for y in range(batch_size)]  # Initialize batch_xs to be filled with zeros
  original_values = [[0 for x in range(n_input)] for y in range(batch_size)]  # Initialize original_values to be filled with zeros
  
  index = 0  # Tracks the index of data_num, the vector of numbers
  
  for i in range(total_batch):
    new_y = 0
    if i == (total_batch - 1):
      new_y = int(data_num_size / n_input) - (batch_size * i)
      if data_num_size % n_input != 0:
        new_y += 1
      batch_xs = [[0 for x in range(n_input)] for y in range(new_y)]
      original_values = [[0 for x in range(n_input)] for y in range(new_y)]
    temp_batch_size = batch_size
    if i == (total_batch - 1):
      temp_batch_size = new_y
    # Put the next (batch_size * n_input) numbers from data_num into batch_xs and the next (batch_size * n_input) numbers from original_data_num into original_values
    for j in range(temp_batch_size):
      for k in range(n_input):
        if index < data_num_size:
          batch_xs[j][k] = data_num[index]
          original_values[j][k] = original_data_num[index]
          index += 1
    
    p = sess.run(y_pred, {X: batch_xs, dropout_prob: 1.0})
    
    # For each predicted value, undo the modification that had been done on the original value of that prediction
    for r in range(np.size(p, 0)):
      for s in range(np.size(p, 1)):
        if ((i * batch_size * n_input) + (r * np.size(p, 1)) + s) < data_num_size:
          p[r][s] = p[r][s] / modifications[((i * batch_size * n_input) + (r * np.size(p, 1)) + s)]
    
    c, d, t = sess.run([cost_orig, delta_orig, y_orig], feed_dict={X: batch_xs, Y: original_values, Z: p, dropout_prob: 1.0})
    
    # Print each input unit index, its t value, its p value, its d value, its relative error, and its cost
    '''print()
    temp_batch_size = batch_size
    if i == (total_batch - 1):
      temp_batch_size = new_y
    for a in range(temp_batch_size):
      for b in range(n_input):
        if(i * batch_size * n_input + a * n_input + b) < data_num_size:
          print("Testing Input Unit %d\tt: %.8f\tp: %.8f\td: %.8f\tError: %.8f\tCost: %.16f" % (((i * batch_size) + a), t[a][b], p[a][b], d[a][b], (abs(d[a][b])/t[a][b]), c))'''
    
    # Print the batch index and its cost
    # print("Batch", i, "- Cost: ", "{:.9f}".format(c), end = '\t')
    for a1 in range(batch_size):
      for b1 in range(n_input):
        if (i * batch_size * n_input + a1 * n_input + b1) < data_num_size:
          error_range[i*batch_size*n_input + a1*n_input+b1] = (abs(d[a1][b1])/t[a1][b1])
          error_sum = error_sum + error_range[i*batch_size*n_input + a1*n_input+b1]
          if(error_range[i*batch_size*n_input + a1*n_input+b1] == 0):
            error_0 = error_0 + 1
          if(0 < error_range[i*batch_size*n_input + a1*n_input+b1] < 0.0001):
            error_A = error_A + 1
          if(0.0001 < error_range[i*batch_size*n_input + a1*n_input+b1] < 0.001):
            error_B = error_B + 1
          if(0.001 < error_range[i*batch_size*n_input + a1*n_input+b1] < 0.01):
            error_C = error_C + 1
          if(0.01 < error_range[i*batch_size*n_input + a1*n_input+b1] < 0.1):
            error_D = error_D + 1
          if(0.1 < error_range[i*batch_size*n_input + a1*n_input+b1] < 1):
            error_E = error_E + 1
          if(1 < error_range[i*batch_size*n_input + a1*n_input+b1]):
            error_F = error_F + 1
  
  print("\nFor the testing data set, Error_0: %.8f\tError_A: %.8f\tError_B: %.8f\tError_C: %.8f\tError_D: %.8f\tError_E: %.8f\tError_F: %.8f\tError_mean: %.8f\t\n" % ((error_0/data_num.size),(error_A/data_num.size),(error_B/data_num.size),(error_C/data_num.size),(error_D/data_num.size),(error_E/data_num.size),(error_F/data_num.size),(error_sum/data_num.size)))
  validation_error = error_sum / data_num.size

print("Training Error: %f" % training_error)
print("Validation Error: %f" % validation_error)
print("Time: %f" % (time.time() - start_time))